LSTM:
  sequence_length: 10
  optimizer: "adam"
  learning_rate: 0.001
  batch_size: 64
  epochs: 15
  patience: 5